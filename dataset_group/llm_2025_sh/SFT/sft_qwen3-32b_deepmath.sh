#!/bin/bash
#SBATCH --job-name=sft_qwen3_32b
#SBATCH --partition=P12
#SBATCH --nodes=1
#SBATCH --gres=gpu:8 # GPUãŒå¿…è¦ãªå ´åˆ
#SBATCH --nodelist=osk-gpu[84]
#SBATCH --cpus-per-task=240
#SBATCH --time=50:00:00

mkdir -p ~/training/sft_Qwen3-32B_deepmath
mkdir -p ~/training/sft_Qwen3-32B_deepmath/checkpoints

cd ~/training/sft_Qwen3-32B_deepmath

#åŸºæœ¬çš„ãªãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è¨­å®š
export NCCL_SOCKET_IFNAME=enp25s0np0
export NVTE_FUSED_ATTN=0
#CUDA_VISIBLE_DEVICESã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ä½¿ç”¨ã™ã‚‹GPUã®æ•°ã‚’åˆ¶å¾¡ã—ã¾ã™ã€‚
#ä¾‹ãˆã°ã€å˜ä¸€GPUã®å ´åˆã¯ä»¥ä¸‹ã®ã‚ˆã†ã«è¨­å®šã—ã¾ã™ï¼š
#export CUDA_VISIBLE_DEVICES=0
export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
ulimit -v unlimited


export WANDB_PROJECT_NAME="competition_sft_qwen3_32b"
export WANDB_RUN_NAME="Qwen3-32B_deepmath"


# ä»¥ä¸‹ã§deepmathã‚’train valã«åˆ†ã‘ã‚‹
# cd /home/Competition2025/P12/P12U025/data/DeepMath-103K-parquet/data

# python - <<'EOF'
# import os
# import sys
# import pandas as pd
# import numpy as np

# # ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹
# base = "/home/Competition2025/P12/P12U025/data/DeepMath-103K-parquet/data"

# # é€£ç•ªãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆtrain-00000-of-00010.parquet ... -00009-of-00010.parquetï¼‰
# files = [os.path.join(base, f"train-000{i:02d}-of-00010.parquet") for i in range(10)]

# # å­˜åœ¨ãƒã‚§ãƒƒã‚¯
# missing = [p for p in files if not os.path.exists(p)]
# if missing:
#     print("âŒ æ¬¡ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ:")
#     for p in missing:
#         print("   ", p)
#     sys.exit(1)
# print("âœ… å…¨ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ã‚’ç¢ºèª")

# # èª­ã¿è¾¼ã¿ï¼†çµåˆï¼ˆãƒ¡ãƒ¢ãƒªã«ä¹—ã‚‹å‰æã€‚å³ã—ã„å ´åˆã¯PyArrowã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚’æ¤œè¨ï¼‰
# dfs = []
# for p in files:
#     print("èª­ã¿è¾¼ã¿ä¸­:", p)
#     # engine='pyarrow' ã‚’æ˜ç¤ºï¼ˆç’°å¢ƒã«åˆã‚ã›ã¦ fastparquet ã§ã‚‚OKï¼‰
#     dfs.append(pd.read_parquet(p, engine="pyarrow"))
# df = pd.concat(dfs, ignore_index=True)
# print(f"â†’ çµåˆå®Œäº†: {len(dfs)} ãƒ•ã‚¡ã‚¤ãƒ«, åˆè¨ˆ {len(df)} è¡Œ")

# # 8:2 ã«ãƒ©ãƒ³ãƒ€ãƒ åˆ†å‰²ï¼ˆå›ºå®šã‚·ãƒ¼ãƒ‰ã§å†ç¾å¯èƒ½ï¼‰
# rng = np.random.default_rng(42)
# perm = rng.permutation(len(df))
# n_train = int(len(df) * 0.8)
# train_idx = perm[:n_train]
# val_idx   = perm[n_train:]

# df_train = df.iloc[train_idx].reset_index(drop=True)
# df_val   = df.iloc[val_idx].reset_index(drop=True)

# # æ›¸ãå‡ºã—ï¼ˆåœ§ç¸®ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®snappyã€‚å¤‰æ›´ã—ãŸã„å ´åˆã¯ compression= ã§ï¼‰
# out_train = os.path.join(base, "train.parquet")
# out_val   = os.path.join(base, "val.parquet")

# df_train.to_parquet(out_train, index=False, engine="pyarrow")
# df_val.to_parquet(out_val, index=False, engine="pyarrow")

# print("âœ… ä½œæˆå®Œäº†")
# print("  -", out_train, f"({len(df_train)} rows)")
# print("  -", out_val,   f"({len(df_val)} rows)")
# EOF


# == ã‚·ãƒ£ãƒ¼ãƒ‰ã”ã¨ã®è¡Œæ•° ==
# train-00000-of-00010.parquet    rows=10,303  size=206.77 MB
# train-00001-of-00010.parquet    rows=10,303  size=202.54 MB
# train-00002-of-00010.parquet    rows=10,302  size=203.72 MB
# train-00003-of-00010.parquet    rows=10,302  size=198.50 MB
# train-00004-of-00010.parquet    rows=10,302  size=197.44 MB
# train-00005-of-00010.parquet    rows=10,302  size=198.12 MB
# train-00006-of-00010.parquet    rows=10,302  size=197.46 MB
# train-00007-of-00010.parquet    rows=10,302  size=197.77 MB
# train-00008-of-00010.parquet    rows=10,302  size=260.77 MB
# train-00009-of-00010.parquet    rows=10,302  size=174.07 MB

# == ã‚·ãƒ£ãƒ¼ãƒ‰åˆè¨ˆ ==
# total_rows (shards) = 103,022

# == å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã®è¡Œæ•° ==
# train.parquet    rows=82,417  size=1628.42 MB
# val.parquet      rows=20,605  size=408.64 MB




#å‹¾é…è“„ç©


torchrun --standalone --nnodes=1 --nproc_per_node=8 \
    -m verl.trainer.fsdp_sft_trainer \
    data.train_files=/home/Competition2025/P12/P12U025/data/DeepMath-103K-parquet/data/train.parquet \
    data.val_files=/home/Competition2025/P12/P12U025/data/DeepMath-103K-parquet/data/val.parquet \
    data.prompt_key=question \
    data.response_key=r1_solution_1 \
    data.train_batch_size=64 \
    data.micro_batch_size_per_gpu=1 \
    data.max_length=8000 \
    +data.dataloader_num_workers=8 \
    model.fsdp_config.model_dtype=bf16 \
    data.truncation=right \
    ++data.filter_overlong_prompts=True \
    model.lora_rank=16 \
    model.lora_alpha=32 \
    model.partial_pretrain=/home/Competition2025/P12/shareP12/models/Qwen3-32B \
    trainer.total_epochs=2 \
    trainer.save_freq=1000 \
    trainer.default_local_dir=$HOME/training/sft_Qwen3-32B_deepmath/checkpoints \
    trainer.logger=['console','wandb'] \
    trainer.project_name=$WANDB_PROJECT_NAME \
    +model.override_config.attn_implementation=flash_attention_2 \
    model.enable_gradient_checkpointing=True \
    ++model.fsdp_config.forward_prefetch=True \
    trainer.experiment_name=$WANDB_RUN_NAME | tee ~/training/sft_Qwen3-32B_deepmath/verl_sft.log


cd $HOME/model/sft_Qwen3-32B_deepmath/checkpoints
ls -la

#å‹¾é…è“„ç©ã—ãŸã„verã“ã‚Œã‚’å¼•æ•°è¿½åŠ 
#     model.enable_gradient_checkpointing=True \
echo "=== SFT Training Completed ==="

# æœ€æ–°ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’è‡ªå‹•æ¤œå‡º
echo "=== Converting to HuggingFace format ==="
LATEST_CHECKPOINT=$(find $HOME/training/sft_Qwen3-32B_deepmath/checkpoints -name "global_step_*" -type d | sort -V | tail -1)

if [ -z "$LATEST_CHECKPOINT" ]; then
    echo "âŒ No checkpoint found!"
    exit 1
fi

echo "Found checkpoint: $LATEST_CHECKPOINT"

# HuggingFaceå½¢å¼ã«å¤‰æ›
python -m verl.model_merger merge \
    --backend fsdp \
    --local_dir $LATEST_CHECKPOINT \
    --target_dir $LATEST_CHECKPOINT/huggingface

echo "=== Uploading to HuggingFace ==="

# é©åˆ‡ãªãƒªãƒã‚¸ãƒˆãƒªåã§ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
huggingface-cli upload \
    Ta1k1/sft_Qwen3-32B-DeepMath \
    $LATEST_CHECKPOINT/huggingface \
    --token $HF_TOKEN

echo "ğŸ‰ Complete! Model uploaded to: https://huggingface.co/Ta1k1/sft_Qwen3-32B-DeepMath"
echo "ğŸ“ Local path: $LATEST_CHECKPOINT/huggingface"
