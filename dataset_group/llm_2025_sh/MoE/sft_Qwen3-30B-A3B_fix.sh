#!/bin/bash
#SBATCH --job-name=sft_qwen8b_deep_math
#SBATCH --partition=P12
#SBATCH --nodes=1
#SBATCH --gres=gpu:8 # GPUãŒå¿…è¦ãªå ´åˆ
#SBATCH --nodelist=osk-gpu[84]
#SBATCH --cpus-per-task=240
#SBATCH --time=30:00:00

#MoEãƒ¢ãƒ‡ãƒ«ã‚’sftã—ã¦ã¿ã‚ˆã†

#srunã™ã‚‹å ´åˆã¯ä»¥ä¸‹ã®ã‚ˆã†ã«å®Ÿè¡Œ
#bash ../shareP12/scancel_hatakeyama.sh gpu85
#srun --partition P12 --nodes=1 --nodelist osk-gpu[85] --gpus-per-node=8  --cpus-per-task=240 --time=30:00:00 --pty bash -i

mkdir -p ~/training/sft_Qwen3-30B-A3B_fix
mkdir -p ~/training/sft_Qwen3-30B-A3B_fix/checkpoints

cd ~/training/sft_Qwen3-30B-A3B_fix

#åŸºæœ¬çš„ãªãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è¨­å®š
export NCCL_SOCKET_IFNAME=enp25s0np0
export NVTE_FUSED_ATTN=0
#CUDA_VISIBLE_DEVICESã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ä½¿ç”¨ã™ã‚‹GPUã®æ•°ã‚’åˆ¶å¾¡ã—ã¾ã™ã€‚
#ä¾‹ãˆã°ã€å˜ä¸€GPUã®å ´åˆã¯ä»¥ä¸‹ã®ã‚ˆã†ã«è¨­å®šã—ã¾ã™ï¼š
#export CUDA_VISIBLE_DEVICES=0
export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
ulimit -v unlimited

#YOU_TEAM ã‚’ wandb ã®çµ„ç¹”åã«ç½®ãæ›ãˆã¦ãã ã•ã„ã€‚

#data.train_files=/home/Competition2025/P12/P12U025/data/DeepMath-103K-parquet/data/train.parque
#ã“ã‚Œã¯
#DeepMath-103Kãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’Parquetå½¢å¼ã§çµåˆã—ã¦å˜ä¸€ã®ãƒ•ã‚¡ã‚¤ãƒ«ã«ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§ã™ã€‚ä»¥ä¸‹å‚è€ƒ
# 
#     cd /home/Competition2025/P12/P12U025/data/DeepMath-103K-parquet/data

#     python - <<'EOF'
#     import os
#     import pandas as pd

#     # ãƒ‘ã‚¹ã‚’çµ„ã¿ç«‹ã¦
#     base = "/home/Competition2025/P12/P12U025/data/DeepMath-103K-parquet/data"
#     files = [os.path.join(base, f"train-000{i:02d}-of-00010.parquet") for i in range(10)]

#     # å­˜åœ¨ãƒã‚§ãƒƒã‚¯
#     missing = [p for p in files if not os.path.exists(p)]
#     if missing:
#         print('âŒ æ¬¡ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ:')
#         for p in missing: print('   ', p)
#         exit(1)
#     print('âœ… å…¨ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ã‚’ç¢ºèª')

#     # èª­ã¿è¾¼ã‚“ã§çµåˆ
#     dfs = []
#     for p in files:
#         print('èª­ã¿è¾¼ã¿ä¸­:', p)
#         dfs.append(pd.read_parquet(p))
#     df = pd.concat(dfs, ignore_index=True)
#     print(f'â†’ çµåˆå®Œäº†: {len(dfs)} ãƒ•ã‚¡ã‚¤ãƒ«, åˆè¨ˆ {len(df)} è¡Œ')

#     # å˜ä¸€ Parquet ã«æ›¸ãå‡ºã—
#     out = "train.parquet"
#     df.to_parquet(out, index=False)
#     print('âœ… ä½œæˆå®Œäº†:', out)
#     EOF
# 


#    data.max_length=16384 \ã“ã‚Œå¤§ãããªã„ã¨ <think>ã‚¿ã‚°ã®ä¸­èº«ã‚’å…¨éƒ¨å«ã‚ã‚Œãªã„ã®ã§æ³¨æ„



export WANDB_PROJECT_NAME="competition_sft_deep_math"
export WANDB_RUN_NAME="Qwen3-30B-A3B_fix"

torchrun --standalone --nnodes=1 --nproc_per_node=8 \
    -m verl.trainer.fsdp_sft_trainer \
    data.train_files=/home/Competition2025/P12/P12U025/data/DeepMath-103K-parquet/data/train.parquet \
    data.val_files=/home/Competition2025/P12/P12U025/data/DeepMath-103K-parquet/data/val.parquet \
    data.prompt_key=question \
    data.response_key=r1_solution_1 \
    data.train_batch_size=64 \
    data.micro_batch_size_per_gpu=1 \
    data.max_length=8000 \
    model.fsdp_config.model_dtype=bf16 \
    data.truncation=right \
    +data.dataloader_num_workers=8 \
    ++data.filter_overlong_prompts=True \
    model.lora_rank=16 \
    model.lora_alpha=32 \
    model.partial_pretrain=/home/Competition2025/P12/P12U025/model/Qwen3-30B-A3B-Base \
    trainer.total_epochs=2 \
    trainer.save_freq=1000 \
    model.enable_gradient_checkpointing=True \
    trainer.default_local_dir=$HOME/training/sft_Qwen3-30B-A3B_fix/checkpoints \
    trainer.logger=['console','wandb'] \
    ++model.fsdp_config.forward_prefetch=True \
    trainer.project_name=$WANDB_PROJECT_NAME \
    trainer.experiment_name=$WANDB_RUN_NAME | tee ~/training/sft_Qwen3-30B-A3B_fix/verl_sft.log

cd $HOME/model/sft_Qwen3-30B-A3B_fix/checkpoints
ls -la

#å‹¾é…è“„ç©ã—ãŸã„verã“ã‚Œã‚’å¼•æ•°è¿½åŠ 
#     model.enable_gradient_checkpointing=True \
echo "=== SFT Training Completed ==="

# æœ€æ–°ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’è‡ªå‹•æ¤œå‡º
echo "=== Converting to HuggingFace format ==="
LATEST_CHECKPOINT=$(find $HOME/training/sft_Qwen3-30B-A3B_fix/checkpoints -name "global_step_*" -type d | sort -V | tail -1)

if [ -z "$LATEST_CHECKPOINT" ]; then
    echo "âŒ No checkpoint found!"
    exit 1
fi

echo "Found checkpoint: $LATEST_CHECKPOINT"

# HuggingFaceå½¢å¼ã«å¤‰æ›
python -m verl.model_merger merge \
    --backend fsdp \
    --local_dir $LATEST_CHECKPOINT \
    --target_dir $LATEST_CHECKPOINT/huggingface

echo "=== Uploading to HuggingFace ==="

# é©åˆ‡ãªãƒªãƒã‚¸ãƒˆãƒªåã§ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
huggingface-cli upload \
    Ta1k1/sft_Qwen3-30B-A3B-SFT-DeepMath \
    $LATEST_CHECKPOINT/huggingface \
    --token $HF_TOKEN

echo "ğŸ‰ Complete! Model uploaded to: https://huggingface.co/Ta1k1/sft_Qwen3-30B-A3B-SFT-DeepMath"
echo "ğŸ“ Local path: $LATEST_CHECKPOINT/huggingface"



